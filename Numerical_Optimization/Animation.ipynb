{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.ticker import LinearLocator\n",
    "import seaborn as sns\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have a dataset or set of data points, the student must generate a pair of arrays <b>X</b> and <b>y</b> with the values in <b>X</b> equally distributed between <b>0</b> and <b>20</b> and the values in <b>y</b> such that: \n",
    "<b>yi = a*xi + b (and a = -1, b = 2)</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(0,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=-1\n",
    "b=2\n",
    "y=a*X+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.normal(0, 1, len(X))\n",
    "y = y + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=X, y=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.hstack((X.reshape((-1,1)), y.reshape((-1,1))))\n",
    "np.random.seed(4)\n",
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_animated(x, y):\n",
    "    ax = plt.plot(x, y, marker='o', color='black', linewidth=0.2, markersize=3, alpha=0.5)\n",
    "    plt.xlabel('Theta 0')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Thetas Vs Loss')\n",
    "    # plt.grid('True')   \n",
    "    plt.xlim(min(x) - 0.2 ,max(x) + 0.2)\n",
    "    plt.ylim(min(y) - 0.2 ,max(y) + 0.2)\n",
    "    for i in range(len(x)):\n",
    "        plt.setp(ax, xdata=x[:i], ydata= y[:i])\n",
    "        plt.pause(0.05)\n",
    "    # plt.close()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Momentum definintion :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Momentum_GD_SingleVariable(data ,alpha, max_num_iterations, gamma, cost_check, grad_check):\n",
    "    X = data[:, 0]\n",
    "    y = data[:, -1]\n",
    "    theta_0, theta_1 = 0,0\n",
    "    loss = [float('inf')]\n",
    "    all_thetas = []\n",
    "    h_xs=[]\n",
    "    m = len(y)\n",
    "    v0, v1 = 0,0\n",
    "\n",
    "\n",
    "    for i in range(max_num_iterations):\n",
    "        print(f'****************** Iteration {i} ********************\\n')\n",
    "\n",
    "        all_thetas.append((theta_0, theta_1))\n",
    "        \n",
    "        h_x = theta_0+theta_1*X\n",
    "        h_xs.append(h_x)\n",
    "        print('h(x) : ', h_x)\n",
    "\n",
    "        error_vector = (h_x - y)\n",
    "        print('\\nError Vector :\\n', error_vector)\n",
    "\n",
    "        MSE = np.sum(error_vector**2) / (2*m)\n",
    "        loss.append(MSE)\n",
    "        print('\\nj = ', MSE)\n",
    "\n",
    "        d_theta_0 = (np.sum(h_x - y))/m\n",
    "        d_theta_1 = (np.sum((h_x - y) *X))/m\n",
    "\n",
    "        gradient_vector = np.array([d_theta_0, d_theta_1])\n",
    "        print('\\nGradient Vector : \\n', gradient_vector)\n",
    "\n",
    "        gradient_vec_norm = np.linalg.norm(gradient_vector)\n",
    "        print('\\n Gradient Vector Norm : ', gradient_vec_norm)\n",
    "    \n",
    "        if i>0:\n",
    "            if gradient_vec_norm <= grad_check or (np.abs(loss[-1] - loss[-2]))<= cost_check:\n",
    "                print('****************** Training Report ********************')\n",
    "                print(f'Gradient Descent converged after {i} iterations')\n",
    "                print('theta_0_Opt : ', theta_0)\n",
    "                print('theta_1_Opt : ', theta_1)\n",
    "                print('\\nError Vector :\\n', error_vector)\n",
    "                print('Cost = ', MSE)\n",
    "                print('h(x) = y_predict: \\n', h_x)\n",
    "                print('y_actual : ', y)\n",
    "\n",
    "                break\n",
    "\n",
    "\n",
    "        v0 = v0*gamma + d_theta_0*alpha\n",
    "        v1 = v1*gamma + d_theta_1*alpha\n",
    "        \n",
    "        theta_0 = theta_0 - v0\n",
    "        theta_1 = theta_1 - v1\n",
    "        print('theta_0_new : ', theta_0)\n",
    "        print('theta_1_new : ', theta_1)\n",
    "        \n",
    "        \n",
    "    return theta_0, theta_1, loss, h_x, i+1, all_thetas, h_xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Mahmoud's Momentum :**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **!! Don't Give a shit to this optimizer I just was trying something ðŸ˜‚**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mod_Momentum_GD_SingleVariable(data ,alpha, max_num_iterations, gamma, cost_check, grad_check):\n",
    "    X = data[:, 0]\n",
    "    y = data[:, -1]\n",
    "    theta_0, theta_1 = 0,0\n",
    "    loss = [float('inf')]\n",
    "    all_thetas = []\n",
    "    h_xs=[]\n",
    "    m = len(y)\n",
    "    v0, v1 = 0,0\n",
    "\n",
    "\n",
    "    for i in range(max_num_iterations):\n",
    "        print(f'****************** Iteration {i} ********************\\n')\n",
    "\n",
    "        all_thetas.append((theta_0, theta_1))\n",
    "        \n",
    "        h_x = theta_0+theta_1*X\n",
    "        h_xs.append(h_x)\n",
    "        print('h(x) : ', h_x)\n",
    "\n",
    "        error_vector = (h_x - y)\n",
    "        print('\\nError Vector :\\n', error_vector)\n",
    "\n",
    "        MSE = np.sum(error_vector**2) / (2*m)\n",
    "        loss.append(MSE)\n",
    "        print('\\nj = ', MSE)\n",
    "\n",
    "        d_theta_0 = (np.sum(h_x - y))/m\n",
    "        d_theta_1 = (np.sum((h_x - y) *X))/m\n",
    "\n",
    "        gradient_vector = np.array([d_theta_0, d_theta_1])\n",
    "        print('\\nGradient Vector : \\n', gradient_vector)\n",
    "\n",
    "        gradient_vec_norm = np.linalg.norm(gradient_vector)\n",
    "        print('\\n Gradient Vector Norm : ', gradient_vec_norm)\n",
    "    \n",
    "        if i>0:\n",
    "            if gradient_vec_norm <= grad_check or (np.abs(loss[-1] - loss[-2]))<= cost_check:\n",
    "                print('****************** Training Report ********************')\n",
    "                print(f'Gradient Descent converged after {i} iterations')\n",
    "                print('theta_0_Opt : ', theta_0)\n",
    "                print('theta_1_Opt : ', theta_1)\n",
    "                print('\\nError Vector :\\n', error_vector)\n",
    "                print('Cost = ', MSE)\n",
    "                print('h(x) = y_predict: \\n', h_x)\n",
    "                print('y_actual : ', y)\n",
    "\n",
    "                break\n",
    "\n",
    "        if loss[-1] > loss[-2] + gamma * abs(loss[-1] -loss[-2]):\n",
    "            gamma = gamma - 0.01\n",
    "        v0 = v0*gamma + d_theta_0*alpha\n",
    "        v1 = v1*gamma + d_theta_1*alpha\n",
    "        \n",
    "        theta_0 = theta_0 - v0\n",
    "        theta_1 = theta_1 - v1\n",
    "        print('theta_0_new : ', theta_0)\n",
    "        print('theta_1_new : ', theta_1)\n",
    "        \n",
    "        \n",
    "    return theta_0, theta_1, loss, h_x, i+1, all_thetas, h_xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **NAG definition:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MiniBatchNAG_Opt(data, batch_size, alpha, gamma, epochs, grad_check=0, cost_check=0):\n",
    "    theta_0 = 0\n",
    "    theta_1 = 0\n",
    "    v_theta_0 = 0  \n",
    "    v_theta_1 = 0  \n",
    "    theta_0_temp = 0\n",
    "    theta_1_temp = 0    \n",
    "    loss = []\n",
    "    y_hat = []\n",
    "    all_thetas = []\n",
    "    costs = [float(\"inf\")]\n",
    "    norms =[]\n",
    "    sh_data = data\n",
    "    np.random.seed(4)\n",
    "    np.random.shuffle(sh_data)\n",
    "    no_of_batches = int(np.ceil(len(sh_data) / batch_size))    \n",
    "\n",
    "    for i in range(epochs):\n",
    "        for j in range(no_of_batches):\n",
    "            mini_batch = sh_data[j * batch_size: (j + 1) * batch_size]\n",
    "            x = mini_batch[:, 0]\n",
    "            y = mini_batch[:, 1]\n",
    "\n",
    "            # theta move with momentum only\n",
    "            theta_0_temp = theta_0 - gamma * v_theta_0\n",
    "            theta_1_temp = theta_1 - gamma * v_theta_1\n",
    "\n",
    "            # use momentum based theta in calculations\n",
    "            h = theta_0_temp + theta_1_temp * x\n",
    "            Err_vec = h - y\n",
    "            J = np.linalg.norm(Err_vec, ord=2) ** 2 / (2 * len(mini_batch))\n",
    "\n",
    "            d_theta_0_temp = np.sum((1 / len(x)) * (h - y))\n",
    "            d_theta_1_temp = np.sum((1 / len(x)) * (h - y) * x)\n",
    "\n",
    "            loss.append(J)\n",
    "            all_thetas.append((theta_0, theta_1))\n",
    "            y_hat.append(h)\n",
    "\n",
    "            # not to use previous thetas in update, but instead use only momentum based thetas\n",
    "            theta_0 = theta_0_temp - alpha * d_theta_0_temp\n",
    "            theta_1 = theta_1_temp - alpha * d_theta_1_temp\n",
    "\n",
    "            v_theta_0 = gamma * v_theta_0 + alpha * d_theta_0_temp\n",
    "            v_theta_1 = gamma * v_theta_1 + alpha * d_theta_1_temp\n",
    "\n",
    "        grad_vec = np.array([d_theta_0_temp, d_theta_1_temp])\n",
    "        norm_grad_vec = np.linalg.norm(x=grad_vec, ord=2)\n",
    "        norms.append(norm_grad_vec)\n",
    "        costs.append(J)\n",
    "        print('*' * 20 + f' Epoch {i} ' + '*' * 20)\n",
    "        print(\"Cost = \", J)\n",
    "        print(\"Gradient Vector : \", grad_vec)\n",
    "        print(\"Gradient Vector norm : \", norm_grad_vec)\n",
    "        print(\"theta_0_new : \", theta_0)\n",
    "        print(\"theta_1_new : \", theta_1)\n",
    "        \n",
    "        if abs(norms[-1]) < grad_check or abs(costs[-1] - costs[-2]) < cost_check:\n",
    "            break\n",
    "\n",
    "    print(\"\\n***** Final Results : (Training Report) *****\\n\")\n",
    "    print(\"theta_0 opt : \", theta_0)\n",
    "    print(\"theta_1 opt : \", theta_1)\n",
    "    print(f\"Nesterov Accelerated Gradient converged after {i + 1} epochs\")\n",
    "    print(\"Cost = \", J)\n",
    "\n",
    "    return all_thetas, loss, y_hat, (theta_0, theta_1), norms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Adam definition:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Adams_Opt(data, betaBias=0.3, batch_size=50, alpha=0.005, beta1=0.9, beta2=0.9, epslon=1e-08, epochs=500, grad_check=0.001, cost_check=0.001):\n",
    "    theta_0 = 0\n",
    "    theta_1 = 0\n",
    "    loss=[]\n",
    "    y_hat = []\n",
    "    all_thetas = []\n",
    "    v_t_0 = 0\n",
    "    v_t_1 = 0\n",
    "    m_t_0 = 0\n",
    "    m_t_1 = 0\n",
    "    if 1 < beta1 < 0 or 1 < beta2 < 0 or  1 < betaBias < 0:\n",
    "        print('Not valid beta')\n",
    "        return\n",
    "    costs = [float(\"inf\")]\n",
    "    norms =[float(\"inf\")]\n",
    "    sh_data = data\n",
    "    np.random.seed(4)\n",
    "    np.random.shuffle(sh_data)\n",
    "    no_of_batches = int(np.ceil(len(sh_data) / batch_size))\n",
    "        \n",
    "    for i in range(epochs):\n",
    "        i += 1\n",
    "        for j in range(no_of_batches):\n",
    "            mini_batch = sh_data[j * batch_size : (j+1) * batch_size]\n",
    "            x = mini_batch[::, 0]\n",
    "            y = mini_batch[::, 1]\n",
    "            \n",
    "            h = theta_0 + theta_1 * x\n",
    "            Err_vec = h - y\n",
    "            \n",
    "            J = np.linalg.norm(Err_vec, ord=2)**2 / (2 * len(mini_batch))\n",
    "            d_theta_0 = np.sum((1 / len(x)) * (Err_vec))\n",
    "            d_theta_1 = np.sum((1 / len(x)) * (Err_vec) * x)\n",
    "            \n",
    "            loss.append(J)\n",
    "            all_thetas.append((theta_0, theta_1))\n",
    "            y_hat.append(h)\n",
    "\n",
    "            m_t_0 = beta1 * m_t_0 + (1 - beta1) * d_theta_0\n",
    "            m_t_1 = beta1 * m_t_1 + (1 - beta1) * d_theta_1\n",
    "            \n",
    "            m_t_0_bias = m_t_0 / (1 - betaBias**(i))\n",
    "            m_t_1_bias = m_t_1 / (1 - betaBias**(i))\n",
    "\n",
    "            \n",
    "            v_t_0 = beta2 * v_t_0 + (1 - beta2) * (d_theta_0)**2\n",
    "            v_t_1 = beta2 * v_t_1 + (1 - beta2) * (d_theta_1)**2\n",
    "\n",
    "            v_t_0_bias = v_t_0 / (1 - betaBias**(i))\n",
    "            v_t_1_bias = v_t_1 / (1 - betaBias**(i))\n",
    "            \n",
    "            theta_0 = theta_0 - (alpha / (np.sqrt(v_t_0_bias) + epslon)) * m_t_0_bias\n",
    "            theta_1 = theta_1 - (alpha / (np.sqrt(v_t_1_bias) + epslon)) * m_t_1_bias\n",
    "            \n",
    "            \n",
    "        grad_vec = np.array([d_theta_0, d_theta_1])\n",
    "        norm_grad_vec = np.linalg.norm(grad_vec, ord=2)\n",
    "        norms.append(norm_grad_vec)\n",
    "        costs.append(J)\n",
    "        print('*' * 20 + f' Epoch {i} ' + '*' * 20)\n",
    "        print(\"Cost = \", J)\n",
    "        print(\"Gradient Vector : \", grad_vec)\n",
    "        print(\"Gradient Vector norm : \", norm_grad_vec)\n",
    "        print(\"theta_0_new : \", theta_0)\n",
    "        print(\"theta_1_new : \", theta_1)\n",
    "    \n",
    "        if abs(norms[-1]) < grad_check or abs(costs[-1] - costs[-2]) < cost_check:\n",
    "            break\n",
    "        \n",
    "    theta_0_opt = theta_0\n",
    "    theta_1_opt = theta_1\n",
    "    print(\"\\n***** Final Results : (Training Report) *****\\n\")\n",
    "    print(\"theta_0 opt : \", theta_0_opt)\n",
    "    print(\"theta_1 opt : \", theta_1_opt)\n",
    "    print(\"Final gradient norm : \", abs(norms[-1]))\n",
    "    print(f\"Gradient Descent converged after {i + 1} epochs\")\n",
    "    print(\"Cost = \", J)\n",
    "\n",
    "        \n",
    "    return all_thetas, loss, y_hat, (theta_0_opt, theta_1_opt)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model parameters :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameters\n",
    "batch_size = 50\n",
    "alpha = 0.01\n",
    "gamma = 0.95\n",
    "epochs = 3000\n",
    "cost_check = 0.0001\n",
    "grad_check = 0.0001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MOMENTUM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_0, theta_1, loss, h_x, i, all_thetas, h_xs = Momentum_GD_SingleVariable(data=data,\n",
    "                                                                              alpha=alpha,\n",
    "                                                                              gamma=gamma,\n",
    "                                                                              max_num_iterations=epochs,\n",
    "                                                                              cost_check=cost_check,\n",
    "                                                                              grad_check=grad_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Mod Momentum :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_0, theta_1, loss, h_x, i, all_thetas, h_xs = Mod_Momentum_GD_SingleVariable(data=data,\n",
    "                                                                              alpha=alpha,\n",
    "                                                                              gamma=gamma,\n",
    "                                                                              max_num_iterations=epochs,\n",
    "                                                                              cost_check=cost_check,\n",
    "                                                                              grad_check=grad_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_thetas, loss, y_hat, (theta_0, theta_1), norms = MiniBatchNAG_Opt(data,\n",
    "                                                                      batch_size=batch_size,\n",
    "                                                                      alpha=alpha,\n",
    "                                                                      gamma=gamma,\n",
    "                                                                      epochs=epochs,\n",
    "                                                                      cost_check=cost_check,\n",
    "                                                                      grad_check=grad_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Adam**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = data\n",
    "batch_size = len(data)\n",
    "beta1 = 0.1\n",
    "beta2 = 0.1\n",
    "alpha = 0.1\n",
    "epochs = 5000\n",
    "grad_check = 0.001\n",
    "cost_check = 0.001\n",
    "epslon = 1e-08\n",
    "\n",
    "all_thetas, loss, y_hat, (theta_0_opt, theta_1_opt) = Adams_Opt(data=data,\n",
    "                                                                batch_size=batch_size,\n",
    "                                                                alpha=alpha,\n",
    "                                                                beta1=beta1,\n",
    "                                                                beta2=beta2,\n",
    "                                                                epochs=epochs,\n",
    "                                                                grad_check=grad_check,\n",
    "                                                                cost_check=cost_check)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Preparing For the graphs :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_0s = np.array(all_thetas)[::, 0]\n",
    "theta_1s = np.array(all_thetas)[::, 1]\n",
    "[Theta_0, Theta_1] = np.meshgrid(np.arange(min(theta_0s), max(theta_0s), 0.01), np.arange(min(theta_1s), max(theta_1s), 0.01))\n",
    "cost = np.zeros_like(Theta_0)\n",
    "loss = []\n",
    "for i in range(Theta_0.shape[0]):\n",
    "    for j in range(Theta_0.shape[1]):\n",
    "        X = data[:, 0]\n",
    "        y = data[:, 1]\n",
    "        h = Theta_0[i,j] + Theta_1[i,j] * X\n",
    "        Err_vec = h - y\n",
    "        J = np.linalg.norm(Err_vec, ord=2) ** 2 / (2 * len(y))\n",
    "        cost[i,j] = J\n",
    "    \n",
    "for th0, th1 in zip(theta_0s, theta_1s):\n",
    "    h = th0 + th1 * X\n",
    "    Err_vec = h - y\n",
    "    J = np.linalg.norm(Err_vec, ord=2) ** 2 / (2 * len(y))\n",
    "    loss.append(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2D-plot to view contours :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "contour = ax.contourf(Theta_0, Theta_1, cost, levels = 10, cmap='coolwarm')\n",
    "ax.set_xlim((min(theta_0s), max(theta_0s)))\n",
    "ax.set_ylim((min(theta_1s), max(theta_1s)))\n",
    "ax.set_xlabel('theta_0')\n",
    "ax.set_ylabel('theta_1')\n",
    "ax.set_title('Contour Plot of Loss Function')\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = plt.colorbar(contour)\n",
    "cbar.set_label('Cost')\n",
    "\n",
    "plt.show()\n",
    "visualize_animated(theta_0s, theta_1s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3D-plot to view gradient move with cost function :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "surf = ax.plot_surface(Theta_0, Theta_1, cost, cmap='coolwarm',\n",
    "                       linewidth=0, antialiased=False, alpha=0.7)\n",
    "ax.set_zlim(cost.min()-5, cost.max()+5)\n",
    "ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "# A StrMethodFormatter is used automatically\n",
    "ax.zaxis.set_major_formatter('{x:.02f}')\n",
    "\n",
    "# Add a color bar which maps values to colors.\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "x_draw = []\n",
    "y_draw = []\n",
    "z_draw = []\n",
    "ax.set_xlim([min(theta_0s), max(theta_0s)])\n",
    "ax.set_ylim([min(theta_1s), max(theta_1s)])\n",
    "for i in range(len(theta_0s)):\n",
    "    x_draw.append(theta_0s[i])\n",
    "    y_draw.append(theta_1s[i])\n",
    "    z_draw.append(loss[i])\n",
    "    line, = ax.plot(x_draw, y_draw, z_draw, 'black')\n",
    "    line.set_data(x_draw, y_draw)\n",
    "    line.set_3d_properties(z_draw)\n",
    "    plt.draw()\n",
    "    plt.pause(0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Show Density of gradient postitions :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAPTOP SHOP\\AppData\\Local\\Temp\\ipykernel_16520\\1189943466.py:1: FutureWarning: \n",
      "\n",
      "`shade` is now deprecated in favor of `fill`; setting `fill=True`.\n",
      "This will become an error in seaborn v0.14.0; please update your code.\n",
      "\n",
      "  sns.kdeplot(x=theta_0s, y=theta_1s, cmap=\"Reds\", shade=True, bw_adjust=.5)\n"
     ]
    }
   ],
   "source": [
    "sns.kdeplot(x=theta_0s, y=theta_1s, cmap=\"Reds\", shade=True, bw_adjust=.5)\n",
    "plt.show()\n",
    "visualize_animated(theta_0s, theta_1s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import *\n",
    "from sympy.plotting import plot3d\n",
    "x,y = symbols('x y')\n",
    "f = ((x-4)**2 + (y-4)**2)**0.01\n",
    "plot3d(f,size=(10,7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
